{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from zntrack import config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "config.nb_name = \"Workflow.ipynb\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 10:11:56.013829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-26 10:11:56.013899: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "import kaggle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from zntrack import Node, NodeConfig, dvc, nodify, utils, zn\n",
    "from zntrack.core import ZnTrackOption"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download the dataset using the `kaggle` package.\n",
    "The dataset can be found at https://www.kaggle.com/datamunge/sign-language-mnist\n",
    "\n",
    "1. Define the function and configure the DVC options via `@nodify`\n",
    "2. call the function without arguments to add it to the DVC graph\n",
    "\n",
    "you might have to set up your kaggle account or download the dataset manually into `dataset/` and skip this Node (by not calling `download_kaggle()`)\n",
    "\n",
    "```python\n",
    "import pathlib\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "kaggle_dir = pathlib.Path.home() / \".kaggle\"\n",
    "kaggle_dir.mkdir(exist_ok=True, parents=True)\n",
    "kaggle_file = kaggle_dir / \"kaggle.json\"\n",
    "\n",
    "username = input()\n",
    "key = getpass.getpass()\n",
    "_ = kaggle_file.write_text(json.dumps({\"username\": username,\"key\":key}))\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-26 10:12:01,748 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n",
      "2022-04-26 10:12:06,623 (WARNING): Running DVC command: 'dvc run -n download_kaggle ...'\n"
     ]
    },
    {
     "data": {
      "text/plain": "NodeConfig(params={'dataset': 'datamunge/sign-language-mnist'}, outs='dataset', outs_no_cache=None, outs_persist=None, outs_persist_no_cache=None, metrics=None, metrics_no_cache=None, deps=None, plots=None, plots_no_cache=None)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@nodify(\n",
    "    outs=\"dataset\",\n",
    "    params={\"dataset\": \"datamunge/sign-language-mnist\"}\n",
    ")\n",
    "def download_kaggle(cfg: NodeConfig):\n",
    "    \"\"\"Download dataset from kaggle\"\"\"\n",
    "    kaggle.api.dataset_download_files(\n",
    "        dataset=cfg.params.dataset, path=cfg.outs, unzip=True\n",
    "    )\n",
    "\n",
    "\n",
    "download_kaggle()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "We use the class based API for a better structure and splitting up the steps into methods.\n",
    "Additionally, we add a method `plot_image` which allows us to look at an arbitrary dataset point."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-26 10:12:14,432 (WARNING): Running DVC command: 'dvc run -n DataPreprocessor ...'\n"
     ]
    }
   ],
   "source": [
    "# zntrack: break\n",
    "class DataPreprocessor(Node):\n",
    "    \"\"\"Prepare kaggle dataset for training\n",
    "\n",
    "    * normalize and reshape the features\n",
    "    * one-hot encode the labels\n",
    "    \"\"\"\n",
    "    # dependencies and parameters\n",
    "    data: pathlib.Path = dvc.deps(pathlib.Path(\"dataset\"))\n",
    "    dataset = zn.params(\"sign_mnist_train\")\n",
    "    # outputs\n",
    "    features: np.ndarray = zn.outs()\n",
    "    labels: np.ndarray = zn.outs()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Primary Node Method\"\"\"\n",
    "        df = pd.read_csv((self.data / self.dataset / self.dataset).with_suffix(\".csv\"))\n",
    "\n",
    "        self.labels = df.values[:, 0]\n",
    "        self.labels = to_categorical(self.labels)\n",
    "        self.features = df.values[:, 1:]\n",
    "\n",
    "        self.normalize_and_scale_data()\n",
    "\n",
    "    def normalize_and_scale_data(self):\n",
    "        self.features = self.features / 255\n",
    "        self.features = self.features.reshape((-1, 28, 28, 1))\n",
    "\n",
    "    def plot_image(self, index):\n",
    "        plt.imshow(self.features[index])\n",
    "        plt.title(f\"Label {self.labels[index].argmax()}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "DataPreprocessor().write_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom ZnTrackOption + Training Model\n",
    "\n",
    "For the model we define a custom ZnTrackOption called `TFModel` which allows us to serialize a TensorFlow model.\n",
    "We use the dvc_option `--outs` and use the zn_type `RESULTS`. These should be the prefered values for most custom serializations.\n",
    "\n",
    "We can overwrite the `get_filename`, `save` and `get_data_from_files` methods as shown to save / load a TensorFlow model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# zntrack: break\n",
    "class TFModel(ZnTrackOption):\n",
    "    dvc_option = \"outs\"\n",
    "    zn_type = utils.ZnTypes.RESULTS\n",
    "\n",
    "    def get_filename(self, instance) -> pathlib.Path:\n",
    "        \"\"\"Filename depending on the instance node_name\"\"\"\n",
    "        return pathlib.Path(\"nodes\", instance.node_name, \"model\")\n",
    "\n",
    "    def save(self, instance):\n",
    "        \"\"\"Serialize and save values to file\"\"\"\n",
    "        model = self.__get__(instance, self.owner)\n",
    "        file = self.get_filename(instance)\n",
    "        model.save(file)\n",
    "\n",
    "    def get_data_from_files(self, instance):\n",
    "        \"\"\"Load values from file and deserialize\"\"\"\n",
    "        file = self.get_filename(instance)\n",
    "        model = keras.models.load_model(file)\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "with this custom Type we can define `model = TFModel()` and use it similar to the other `zn.<options>` but passing it a TensorFlow model.\n",
    "Note: You can also register a custom `znjson` de/serializer and use `zn.outs` instead.\n",
    "\n",
    "In this simple example we only define the epochs as parameters. For a more advanced Node you would try to catch all parameters, such as layer types, neurons, ... as `zn.params`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-26 10:12:21,704 (WARNING): Running DVC command: 'dvc run -n MLModel ...'\n"
     ]
    }
   ],
   "source": [
    "class MLModel(Node):\n",
    "    # dependencies\n",
    "    train_data: DataPreprocessor = zn.deps(DataPreprocessor)\n",
    "    # outputs\n",
    "    training_history = zn.plots()\n",
    "    metrics = zn.metrics()\n",
    "    # custom model output\n",
    "    model = TFModel()\n",
    "    # parameter\n",
    "    epochs = zn.params()\n",
    "    filters = zn.params([4])\n",
    "    dense = zn.params([4])\n",
    "\n",
    "    def __init__(self, epochs: int = 3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.optimizer = \"adam\"\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Primary Node Method\"\"\"\n",
    "        self.build_model()\n",
    "        self.train_model()\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        print(self.model.summary())\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.train_data.features,\n",
    "            self.train_data.labels,\n",
    "            validation_split=0.3,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=64,\n",
    "        )\n",
    "        self.training_history = pd.DataFrame(history.history)\n",
    "        self.training_history.index.name = \"epoch\"\n",
    "        # use the last values for model metrics\n",
    "        self.metrics = dict(self.training_history.iloc[-1])\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build the model using keras.Sequential API\"\"\"\n",
    "\n",
    "        inputs = keras.Input(shape=(28, 28, 1))\n",
    "        cargo = inputs\n",
    "        for filters in self.filters:\n",
    "            cargo = layers.Conv2D(\n",
    "                filters=filters, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "            )(cargo)\n",
    "            cargo = layers.MaxPooling2D((2, 2))(cargo)\n",
    "\n",
    "        cargo = layers.Flatten()(cargo)\n",
    "\n",
    "        for dense in self.dense:\n",
    "            cargo = layers.Dense(dense, activation=\"relu\")(cargo)\n",
    "\n",
    "        output = layers.Dense(25, activation=\"softmax\")(cargo)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "MLModel().write_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process Test Data\n",
    "\n",
    "We haven't processed our test data yet. We can use the same `DataPreprocessor` Node that we defined previously but give it a different name and pass the test dataset as parameter instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-26 10:12:29,950 (WARNING): Running DVC command: 'dvc run -n data_preprocess_test ...'\n"
     ]
    }
   ],
   "source": [
    "DataPreprocessor(dataset=\"sign_mnist_test\", name=\"data_preprocess_test\").write_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the Model\n",
    "We define an additional Node to evaluate the model against the test data. Here we use the `DataPreprocessor` as dependency.\n",
    "Because we gave the test data Node a special name we can not use `DataPreprocessor` but must use `DataPreprocessor.load(name=<nodename>)` instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-26 10:12:37,620 (WARNING): Running DVC command: 'dvc run -n EvaluateModel ...'\n",
      "2022-04-26 10:12:41,422 (WARNING): Running DVC command: 'dvc plots modify nodes/EvaluateModel/confusion_matrix.csv ...'\n"
     ]
    }
   ],
   "source": [
    "# zntrack: break\n",
    "class EvaluateModel(Node):\n",
    "    # dependencies\n",
    "    ml_model: keras.Model = zn.deps(MLModel @ \"model\")\n",
    "    test_data: DataPreprocessor = zn.deps()\n",
    "    # metrics\n",
    "    metrics = zn.metrics()\n",
    "    confusion_matrix = zn.plots(template=\"confusion\",x=\"predicted\", y=\"actual\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Primary Node Method\"\"\"\n",
    "        loss, accuracy = self.ml_model.evaluate(\n",
    "            self.test_data.features, self.test_data.labels\n",
    "        )\n",
    "        self.metrics = {\"loss\": loss, \"accuracy\": accuracy}\n",
    "\n",
    "        prediction = self.ml_model.predict(self.test_data.features)\n",
    "\n",
    "        self.confusion_matrix = pd.DataFrame([{\"actual\": np.argmax(true), \"predicted\": np.argmax(false)} for true, false in zip(self.test_data.labels, prediction)])\n",
    "\n",
    "EvaluateModel(\n",
    "    test_data=DataPreprocessor[\"data_preprocess_test\"]\n",
    ").write_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Graph\n",
    "We can have a brief look at the generated DAG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                +-----------------+                    \r\n",
      "                | download_kaggle |                    \r\n",
      "                +-----------------+                    \r\n",
      "                 ***            ***                    \r\n",
      "               **                  **                  \r\n",
      "             **                      **                \r\n",
      "+------------------+                   **              \r\n",
      "| DataPreprocessor |                    *              \r\n",
      "+------------------+                    *              \r\n",
      "          *                             *              \r\n",
      "          *                             *              \r\n",
      "          *                             *              \r\n",
      "    +---------+             +----------------------+   \r\n",
      "    | MLModel |             | data_preprocess_test |   \r\n",
      "    +---------+**           +----------------------+   \r\n",
      "                 ***            ***                    \r\n",
      "                    **        **                       \r\n",
      "                      **    **                         \r\n",
      "                 +---------------+                     \r\n",
      "                 | EvaluateModel |                     \r\n",
      "                 +---------------+                     \r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!dvc dag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}